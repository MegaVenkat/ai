from nltk.tokenize import word_tokenize
text = " Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition and machine vision."
print("\nOriginal string:")
print(text)
print("\nList of words:")
print(word_tokenize(text))
